{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "X-BQSMjjrt7r"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.kill(os.getpid(), 9) # Esta linha serve para forçar a reinicialização do processo do notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NaAh-Par_LNo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_GmjTd0xA-n",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nVSLKwUxxEQn"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import plotly.express as px\n",
    "\n",
    "torch.manual_seed(10)\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ray[rllib]\n",
    "\n",
    "A biblioteca RAY possui diversos [algoritmos](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html) de reforço já implementados. Ela busca simplificar a execução e configuração destes algoritmos com objetos de configuração como mostrado abaixo:"
   ],
   "metadata": {
    "id": "uCFoQYae7S7B"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "l_6a5sY7ryWp"
   },
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo.ppo import PPOConfig\n",
    "ppo_config = PPOConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "AS configurações são separadas em seções. Algumas são comuns, outras específicas de cada algoritmo. Na célula abaixo temos uma confiuguração do algoritmo PPO. Nem todas as opções estão presentes pois há várias que são muito específicas. Após ler as configurações da célula abaixo, leia, por cima, as configurações na [documentação do RAY](https://docs.ray.io/en/latest/rllib/rllib-training.html#configuring-rllib-algorithms)."
   ],
   "metadata": {
    "id": "vD2a6Kp_8EGG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from soccer_twos import EnvType\n",
    "import gymnasium as gym\n",
    "from ray import tune\n",
    "from ray.rllib import MultiAgentEnv\n",
    "import soccer_twos\n",
    "from ray.tune.logger import pretty_print"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class RLLibWrapper(gym.core.Wrapper, MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    A RLLib wrapper so our env can inherit from MultiAgentEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_rllib_env(env_config: dict = {}):\n",
    "    \"\"\"\n",
    "    Creates a RLLib environment and prepares it to be instantiated by Ray workers.\n",
    "    Args:\n",
    "        env_config: configuration for the environment.\n",
    "            You may specify the following keys:\n",
    "            - variation: one of soccer_twos.EnvType. Defaults to EnvType.multiagent_player.\n",
    "            - opponent_policy: a Callable for your agent to train against. Defaults to a random policy.\n",
    "    \"\"\"\n",
    "    if hasattr(env_config, \"worker_index\"):\n",
    "        env_config[\"worker_id\"] = (\n",
    "            env_config.worker_index * env_config.get(\"num_envs_per_worker\", 1)\n",
    "            + env_config.vector_index\n",
    "        )\n",
    "    env = soccer_twos.make(**env_config)\n",
    "    # env = TransitionRecorderWrapper(env)\n",
    "    if \"multiagent\" in env_config and not env_config[\"multiagent\"]:\n",
    "        # is multiagent by default, is only disabled if explicitly set to False\n",
    "        return env\n",
    "    return RLLibWrapper(env)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "tune.registry.register_env(\"Soccer\", create_rllib_env)  # registrando o ambiente no tune"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1DYDh7nbrzmi"
   },
   "outputs": [],
   "source": [
    "environment_id = \"Soccer\"\n",
    "\n",
    "ppo_config = ppo_config.resources(\n",
    "    num_gpus = 1,\n",
    "    num_cpus_per_worker = 0,\n",
    ")\n",
    "\n",
    "ppo_config = ppo_config.rollouts(\n",
    "    num_rollout_workers = 8,\n",
    "    # Number of rollout worker actors to create for parallel sampling. Setting this to 0 will force rollouts to be done in the local worker (driver process or the Algorithm’s actor when using Tune).\n",
    "    num_envs_per_worker = 2,\n",
    "    # Number of environments to evaluate vector-wise per worker. This enables model inference batching, which can improve performance for inference bottlenecked workloads.\n",
    "    rollout_fragment_length = 8,\n",
    "    # Divide episodes into fragments of this many steps each during rollouts.\n",
    ")\n",
    "\n",
    "ppo_config = ppo_config.environment(\n",
    "    env = environment_id,\n",
    ")\n",
    "\n",
    "ppo_config.env_config = {\"render\": False, \"time_scale\": 50, \"multiagent\": False, \"variation\": EnvType.team_vs_policy,\n",
    "                         \"flatten_branched\": True, \"single_player\": True}  # colocando os parâmertros do ambiente (soccer-twos)\n",
    "\n",
    "ppo_config = ppo_config.framework(\n",
    "    framework = \"torch\",\n",
    ")\n",
    "\n",
    "ppo_config = ppo_config.debugging(\n",
    "    seed = 10,\n",
    ")\n",
    "\n",
    "ppo_config = ppo_config.training(\n",
    "    lr = 5e-4,\n",
    "    #  The default learning rate.\n",
    "    train_batch_size = 256, # deve ser múltiplo de (workers * env_per_worker * rollout_fragment_length)\n",
    "    # Training batch size, if applicable.\n",
    "    use_critic = True,\n",
    "    # Should use a critic as a baseline (otherwise don’t use value baseline; required for using GAE).\n",
    "    use_gae = True,\n",
    "    # If true, use the Generalized Advantage Estimator (GAE) with a value function, see https://arxiv.org/pdf/1506.02438.pdf.\n",
    "    gamma = 0.99,\n",
    "    # Float specifying the discount factor of the Markov Decision process.\n",
    "    lambda_ = 0.95,\n",
    "    # The GAE (lambda) parameter.\n",
    "    sgd_minibatch_size = 32,\n",
    "    # Total SGD batch size across all devices for SGD. This defines the minibatch size within each epoch.\n",
    "    num_sgd_iter = 6,\n",
    "    # Number of SGD iterations in each outer loop (i.e., number of epochs to execute per train batch).\n",
    "    shuffle_sequences = True,\n",
    "    # Whether to shuffle sequences in the batch when training (recommended).\n",
    "    vf_loss_coeff = 0.5,\n",
    "    # Coefficient of the value function loss. IMPORTANT: you must tune this if you set vf_share_layers=True inside your model’s config.\n",
    "    entropy_coeff = 0.0,\n",
    "    # Coefficient of the entropy regularizer.\n",
    "    vf_clip_param = 100000.0, #Aqui eliminamos o clip colocando ele muito alto\n",
    "    # Clip param for the value function. Note that this is sensitive to the scale of the rewards. If your expected V is large, increase this.\n",
    "    clip_param = 0.5,\n",
    "    #  PPO clip parameter.\n",
    "    kl_coeff = 0.0,\n",
    "    # Initial coefficient for KL divergence.\n",
    "    model = {\n",
    "        \"fcnet_hiddens\": [64, 32],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "        \"vf_share_layers\": False,\n",
    "    },\n",
    "    # Arguments passed into the policy model. See models/catalog.py for a full list of the available model options. \n",
    ")\n",
    "\n",
    "ppo_config = ppo_config.reporting(\n",
    "    min_sample_timesteps_per_iteration = 1,\n",
    "    metrics_num_episodes_for_smoothing = 50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aqui aplicamos a configuração para construir o algoritmo/agente."
   ],
   "metadata": {
    "id": "hOmUjEOL8xw8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKaTv-2irzq0",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "PPOalgo = ppo_config.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Em um passo de treinamento o RAY automaticamente cria os \"workers\" e dispara a coleta de experiências. Coletadas o suficiente, a biblioteca executa um passo de treino nos dados adquiridos e retorna um dicionário de resultados. Leia atentamente a saída da célula a seguir para que você possa se familiarizar com as métricas coletadas."
   ],
   "metadata": {
    "id": "YOydpa-e85Tt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXP34tMPrzwZ"
   },
   "outputs": [],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "result = PPOalgo.train()\n",
    "print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Após a execução do algoritmo, para liberarmos os recursos de cpu e gpu, é preciso chamar o `ray.shutdown()`"
   ],
   "metadata": {
    "id": "ZwaBd4oA9WAF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udlTAXFyB4l2"
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PPO\n",
    "\n",
    "Aqui temos uma configuração e execução do algoritmo PPO para o ambiente do [CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)"
   ],
   "metadata": {
    "id": "yWybVl3D9hXW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ek85TUZ4MKTV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "MAX_EPISODES = 1000\n",
    "\n",
    "all_metrics = pd.DataFrame()\n",
    "all_metrics[\"episodes\"] = [i+1 for i in range(MAX_EPISODES)]\n",
    "all_metrics[\"threshold_reward\"] = [475 for i in range(MAX_EPISODES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJo1cr9GVgAK"
   },
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo.ppo import PPOConfig\n",
    "ppo_config = PPOConfig()\n",
    "\n",
    "environment_id = \"CartPole-v1\"\n",
    "\n",
    "ppo_config = ppo_config.resources(\n",
    "    num_gpus = 0,\n",
    "    num_cpus_per_worker = 0,\n",
    ")\n",
    "ppo_config = ppo_config.rollouts(\n",
    "    num_rollout_workers = 4, \n",
    "    # Number of rollout worker actors to create for parallel sampling. Setting this to 0 will force rollouts to be done in the local worker (driver process or the Algorithm’s actor when using Tune).\n",
    "    num_envs_per_worker = 2,\n",
    "    # Number of environments to evaluate vector-wise per worker. This enables model inference batching, which can improve performance for inference bottlenecked workloads.\n",
    "    rollout_fragment_length = 8,\n",
    "    # Divide episodes into fragments of this many steps each during rollouts.\n",
    ")\n",
    "ppo_config = ppo_config.environment(\n",
    "    env = environment_id,\n",
    ")\n",
    "ppo_config = ppo_config.framework(\n",
    "    framework = \"torch\",\n",
    ")\n",
    "ppo_config = ppo_config.debugging(\n",
    "    seed = 10,\n",
    ")\n",
    "ppo_config = ppo_config.training(\n",
    "    \n",
    "    lr = 0.0005,\n",
    "    #  The default learning rate.\n",
    "    train_batch_size = 128, # deve ser múltiplo de (workers * env_per_worker * rollout_fragment_length)\n",
    "    # Training batch size, if applicable.\n",
    "    use_critic = True,\n",
    "    # Should use a critic as a baseline (otherwise don’t use value baseline; required for using GAE).\n",
    "    use_gae = True,\n",
    "    # If true, use the Generalized Advantage Estimator (GAE) with a value function, see https://arxiv.org/pdf/1506.02438.pdf.\n",
    "    gamma = 0.99,\n",
    "    # Float specifying the discount factor of the Markov Decision process.\n",
    "    lambda_ = 0.95,\n",
    "    # The GAE (lambda) parameter.\n",
    "    sgd_minibatch_size = 32,\n",
    "    # Total SGD batch size across all devices for SGD. This defines the minibatch size within each epoch.\n",
    "    num_sgd_iter = 6,\n",
    "    # Number of SGD iterations in each outer loop (i.e., number of epochs to execute per train batch).\n",
    "    shuffle_sequences = True,\n",
    "    # Whether to shuffle sequences in the batch when training (recommended).\n",
    "    vf_loss_coeff = 0.5,\n",
    "    # Coefficient of the value function loss. IMPORTANT: you must tune this if you set vf_share_layers=True inside your model’s config.\n",
    "    entropy_coeff = 0.0,\n",
    "    # Coefficient of the entropy regularizer.\n",
    "    vf_clip_param = 100000.0, #Aqui eliminamos o clip colocando ele muito alto\n",
    "    # Clip param for the value function. Note that this is sensitive to the scale of the rewards. If your expected V is large, increase this.\n",
    "    clip_param = 0.5,\n",
    "    #  PPO clip parameter.\n",
    "    kl_coeff = 0.0,\n",
    "    # Initial coefficient for KL divergence.\n",
    "    model = {\n",
    "        \"fcnet_hiddens\": [64, 32],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "        \"vf_share_layers\": False,\n",
    "    },\n",
    "    # Arguments passed into the policy model. See models/catalog.py for a full list of the available model options. \n",
    ")\n",
    "\n",
    "ppo_config = ppo_config.reporting(\n",
    "    min_sample_timesteps_per_iteration = 1,\n",
    "    metrics_num_episodes_for_smoothing = 50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t69KwHF8C-f2"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def run_experiment(name, config):\n",
    "    PPOalgo = config.build()\n",
    "\n",
    "    metrics = defaultdict(list)\n",
    "\n",
    "    rew_deque = deque(maxlen=50)\n",
    "    len_deque = deque(maxlen=50)\n",
    "\n",
    "    pbar = tqdm(total=MAX_EPISODES, position=0, leave=True)\n",
    "\n",
    "    episode = 0\n",
    "    while episode < MAX_EPISODES:\n",
    "        result = PPOalgo.train()\n",
    "\n",
    "    # O código abaixo é feito para adquirir métricas com vários episódios coletados e terminados na chamada de 'train()'\n",
    "    # ele serve como um exemplo do uso da métricas na variável 'result'\n",
    "        if result[\"episodes_total\"] > episode:\n",
    "            for v in result[\"hist_stats\"][\"episode_reward\"][-result[\"sampler_results\"][\"episodes_this_iter\"]:]:\n",
    "                rew_deque.append(v)\n",
    "                metrics[\"train_reward\"].append(np.array(rew_deque).mean())\n",
    "\n",
    "            for v in result[\"hist_stats\"][\"episode_lengths\"][-result[\"sampler_results\"][\"episodes_this_iter\"]:]:\n",
    "                len_deque.append(v)\n",
    "                metrics[\"ep_len\"].append(np.array(len_deque).mean())\n",
    "            \n",
    "            pbar.update(result[\"episodes_total\"] - episode)\n",
    "            pbar.set_description(\"| Mean Reward %.2f | Ep len %.2f |\" % (result[\"sampler_results\"][\"episode_reward_mean\"], result[\"sampler_results\"][\"episode_len_mean\"]))\n",
    "\n",
    "            episode = result[\"episodes_total\"]\n",
    "\n",
    "    all_metrics[name+\"_reward\"] = metrics[\"train_reward\"][:1000]\n",
    "    all_metrics[name+\"_len\"] = metrics[\"ep_len\"][:1000]\n",
    "    \n",
    "    # !!!!! Esta execução dura ~5min\n",
    "    return PPOalgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAYPkFyr7Mj3"
   },
   "outputs": [],
   "source": [
    "agent = run_experiment(name=\"PPO_01\", config=ppo_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp-NAnBH79qt"
   },
   "source": [
    "# TensorBoard\n",
    "\n",
    "O RAY automaticamente guarda as métricas da variável \"result\" em uma pasta no formato de csv e para o tensorboard. Execute as células a seguir e explore os resultados no ambiente do TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dj7QIO__SChA"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "log_folder = \"/root/ray_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmlpap4NqnH3"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir={log_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para facilidade de visualização e entrega do notebook, utilizaremos ainda o plotly para observar as métricas de recompensa e tamanho de episódio.\n",
    "\n",
    "`Obs: É normal não atingir o limiar ainda`"
   ],
   "metadata": {
    "id": "DomOyqil-C-j"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H613mdTTDcie"
   },
   "outputs": [],
   "source": [
    "px.line(all_metrics, x=\"episodes\", y=[col for col in all_metrics.columns if '_reward' in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAWi2DMoDdpz"
   },
   "outputs": [],
   "source": [
    "px.line(all_metrics, x=\"episodes\", y=[col for col in all_metrics.columns if '_len' in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBMdnGPd9cnp"
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercício 1\n",
    " Analise as curvas de recompensa e tamanho do episódio ressaltando similaridades e diferenças entre elas. Explique o porque destas similaridades e diferenças."
   ],
   "metadata": {
    "id": "bRHKll38-23p"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "eze9Du3S-4gB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercício 2\n",
    "\n",
    "A configuração anterior não foi capaz de passar do limiar de recompensa estabelecido. Altere a configuração da cálula abaixo para que ela atija o limiar antes do episódio `700`.\n",
    "\n",
    "ATENÇÃO:\n",
    "* Não altere a `seed`\n",
    "* O Google Colab fornece apenas dois núcleos de CPU, aumentar muito o número de *workers* pode afetar significativamente o tempo de execução. "
   ],
   "metadata": {
    "id": "rDJb9Y1n_loX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsrlYoZI5YE6"
   },
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo.ppo import PPOConfig\n",
    "ppo_config = PPOConfig()\n",
    "\n",
    "environment_id = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-Dvs3gw8nky"
   },
   "outputs": [],
   "source": [
    "agent = run_experiment(name=\"PPO_02\", config=ppo_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNKeHJFW8ndS"
   },
   "outputs": [],
   "source": [
    "px.line(all_metrics, x=\"episodes\", y=[col for col in all_metrics.columns if '_reward' in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1iy_6RkErLu"
   },
   "outputs": [],
   "source": [
    "px.line(all_metrics, x=\"episodes\", y=[col for col in all_metrics.columns if '_len' in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qda7iiurGF99"
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercício 3\n",
    "\n",
    "Detalhe QUAIS foram as alterações feitas na configuração, e EXPLIQUE por que elas ajudaram o algoritmo a convergir inserindo o papel de cada uma no PPO."
   ],
   "metadata": {
    "id": "fpvkXvSG_7JC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iP7OUTSjGlSE"
   },
   "outputs": [],
   "source": [
    "# Explicar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3KmCU7T7Mj-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
